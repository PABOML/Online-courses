---
title: "Random Forest"
author: "Bert Gollnick"
date: "30 November 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)  # for data manipulation
library(randomForest)  # for random forest model creation
library(keras)  # for multiassignment operation
library(caret)  # for model performance
library(reshape2)
source("./functions/train_val_test.R")
```


# Business Understanding

We will create a Credit Approval model. More information can be found at this [link](http://archive.ics.uci.edu/ml/datasets/credit+approval).

We will predict whether an applicant received a credit card.

# Data Understanding

```{r}
credit_approval <- readRDS("./data/CreditApproval.RDS")
```

The dataset has `r nrow(df)` observations and `r ncol(df)-1` attributes, plus one class attribute. The class attribute   

# Data Preparation

In tutorial on missing data handling, this dataset was prepared and missing data was taken care of.

```{r}
credit_approval %>% summary()
```

The class attribute has values "-" and "+" which will be transformed to 0 and 1.

```{r}
credit_approval <- credit_approval %>% 
  dplyr::mutate(Approved = ifelse(Approved == "+", 1, 0)) %>%
  dplyr::mutate(Income = log(Income))

credit_approval$Income[is.infinite(credit_approval$Income)] <- 0

```

We check that we have the same distribution as before.

```{r}
credit_approval$Approved %>% table()
```

We change type of "Approved" to factor.
```{r}
credit_approval$Approved <- as.factor(credit_approval$Approved)
```


## Train / Validation Split

For simplicity we will only split into training and validation

```{r}
c(train, val, test) %<-% train_val_test_split(df = credit_approval, train_ratio = 0.8, val_ratio = 0.2, test_ratio = 0)
```


# Modeling

```{r}
model_rf <- randomForest(data = train, Approved ~ .)
model_rf
```

# Predictions

```{r}
val$Approved_pred <- predict(model_rf, val)
```

# Model Performance

## Confusion Matrix

```{r}
conf_mat <- table(predicted = val$Approved_pred, actual = val$Approved)
conf_mat
```

```{r}
caret::confusionMatrix(conf_mat)
```

Here you can see many important model performance metrics on one glance.

Accuracy is 89 %, which is quite good.

## Feature Importance

```{r}
feat_importance <- model_rf %>% 
  randomForest::importance()
feat_importance <- tibble(feature = rownames(feat_importance),
                          importance = feat_importance[, 1])
# sort the featues
feat_importance$feature <- feat_importance$feature %>% 
  fct_reorder(feat_importance$importance)
```

```{r}
g <- ggplot(feat_importance, aes(x = feature,
                                 y = importance))
g <- g + geom_col()
g
```

Prior default has the highest importance of all features.

A simple graph can also be plotted directly with *varImpPlot()* function.

```{r}
varImpPlot(model_rf)
```

# Model Parameter Tuning

RandomForests have some hyperparameters that can be 

```{r}
ntree_range <- seq(10,200, 10)
ntree_res <- tibble(range = ntree_range,
                    accuracy = NA)
for (ntree in ntree_range) {
  # Create the model with hypertuning parameter
  model_rf <- randomForest(data = train, Approved ~ ., ntree = ntree)

  # create predictions on validation dataset
  val$Approved_pred <- predict(model_rf, val)

  # create confusion matrix
  conf_mat <- table(predicted = val$Approved_pred, actual = val$Approved)

  # derive metrics from confusion matrix
  acc <- caret::confusionMatrix(conf_mat)$overall[1]
  ntree_res$accuracy[which(ntree_range == ntree)] <- acc
}

g <- ggplot(ntree_res, aes(range, accuracy))
g <- g + geom_line()
g
```

```{r}
mtry_range <- seq(2,10, 1)
mtry_res <- tibble(range = mtry_range,
                    accuracy = NA)
for (mtry in mtry_range) {
  # Create the model with hypertuning parameter
  model_rf <- randomForest(data = train, 
                           Approved ~ ., 
                           ntree = 100, 
                           mtry = mtry)

  # create predictions on validation dataset
  val$Approved_pred <- predict(model_rf, val)

  # create confusion matrix
  conf_mat <- table(predicted = val$Approved_pred, actual = val$Approved)

  # derive metrics from confusion matrix
  acc <- caret::confusionMatrix(conf_mat)$overall[1]
  mtry_res$accuracy[which(mtry_range == mtry)] <- acc
}

g <- ggplot(mtry_res, aes(range, accuracy))
g <- g + geom_line()
g

```

